直推式推理（transductive）-》归纳式推理

（shallow embedding）浅层编码器=》复杂编码器（图神经网络）
浅层编码器的局限
复杂编码器 图神经网络（GCN，GraphSAGE，**GAT**，R-GCN
模型要有 置换不变性，置换同变性
简单直觉的图网络可以直接使用MLP（多层感知器）对邻接矩阵表示的图处理，但是不一定满足置换不变性和置换同变性
GNN 神经消息传递机制，矢量消息在节点之间交换，两步走
1. 聚合邻居的信息
2. 使用邻居信息更新自己的信息
但是第一个节点的信息还是要使用shallow embedding
图结构、邻居节点特征
GNN层数越深，模型感受野越大，导致模型过平滑，各个节点特别接近。
消息传递的表达式有图级别的表示方式和 节点级别的表示方式
自环消息传递，把自己的信息放在了聚合阶段，不用再更新本节点，但这样限制了GNN的表达能力，W_self和W_neigh共享参数，降低了模型参数量，

### 邻域归一化：

平均归一化、对称归一化
归一化可能倒是信息丢失，对节点的度不敏感，掩盖结构特征。
节点特征信息比结构信息更有效，节点差异度过大导致优化过程不稳定，这时候需要做归一化

## GCN图卷积神经网络

使用对称归一化+自环消息传递

### 集合池化：MLP套MLP

性能提高，过拟合的风险增加，经验上来说一层MLP就够了
集合池化：使用置换不变函数（求和，平均，最大值等）
Janossy池化：不使用置换不变函数，使用多个置换不敏感的函数，对多个可能的置换取平均
Janossy性能优于集合池化

### GAT模型 邻域注意力模型

为每个邻域中的节点分配注意力权重，用于权衡节点在聚合步骤中的影响力

### GNN与Transformer

what is Transformer ？ https://zhuanlan.zhihu.com/p/48508221

### 解决过度平滑

拼接+跳跃连接

### NLP

TF-IDE

#### 图构建

静态图构建
输入原始文本，输出图，注入领域知识帮助模型理解文本
依赖图，dependency Graph 解析成词
依赖解析工具（denpendency Prasing) NLTK
constituency parsing 解析成片段
