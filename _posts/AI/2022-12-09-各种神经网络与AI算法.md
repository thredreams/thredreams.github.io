哔哩哔哩视频： https://www.bilibili.com/video/av28733156/?p=3

### ANN 人工神经网络
人工神经网络或者也可以直接代指神经网络。
多层感知模型（MLP）-真正意义上的神经网络
多层感知器模型（也称为全连接神经网络）是真正意义上的神经网络，可看作是logistic回归的推广。网络由输入层，输出层，以及隐含层构成。任意两个相邻的层的所有神经元之间都有连接关系。其隐含层的神经元实现的正是logistic回归的映射。

![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACnMI5Cmd6oPHC87H5CP7jfFjasJibH8IPY2aPnvJ4rUCvzyjt5TIuLMhoc4vddHvic5KibYLT7Bykj8A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

本质上看，神经网络是一个多层复合函数。万能逼近定理保证了只有一个隐含层的神经网络就可以逼近闭区间上的任意连续函数。神经网络的训练一般采用著名的反向传播算法，早在1986年就被提出。关于神经网络的原理，可以阅读SIGAI之前的公众号文章“理解神经网络的激活函数”，“反向传播算法推导-全连接神经网络”。系统的阐述可以阅读[《机器学习与应用》](http://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247488227&idx=2&sn=b6672c6b313a82415992e3b56e817720&chksm=fdb68f74cac10662fea3bd9dc83c7aebd0243967d8cc69403c3c23a0d29f572a2a5b56c6fee9&scene=21#wechat_redirect)一书。  

如果将神经网络的层抽象成一个节点，则多层感知器模型是一个线性结构，每个节点最多只和其前驱节点，后续节点有连接关系。而深度学习后续的发展突破了这一限制，除循环神经网络之外，只要是无孤立节点的有向无环图，均为一个合法的神经网络结构。
前馈神经网络是第一种也是最简单的人工神经网络设计。在该网络中，信息仅在一个方向上移动，从输入节点向前移动，通过隐藏节点（如果有的话）移动到输出节点。网络中没有循环或循环。
参考链接：
https://mp.weixin.qq.com/s/z_WbYTR1XiHusGr5lj2XOA
#### 神经元
将输入的向量加权求和转置，然后使用激活函数得到一个输出
-   **逻辑回归就是激活函数是sigmoid的单层简单神经网络**。也就是说，只要在神经元的输出端有一个合适的损失函数，就能让单个神经元变成一个线性分类器。因此说，那些线性的分类器本身就是一个单层神经网络
-   但注意，对于非线性的模型：**SVM和神经网络走了两条不同的道路**：神经网络通过多个隐层的方法来实现非线性的函数，有一些理论支持（比如说带隐层的神经网络可以模拟任何函数），但是目前而言还不是非常完备；SVM则采用了kernel trick的方法，这个在理论上面比较完备（RKHS，简单地说就是一个泛函的线性空间）。两者各有好坏，神经网络最近的好处是网络设计可以很灵活，有很多的trick&tip，很多理论都不清不楚的；SVM的理论的确漂亮，但是kernel设计不是那么容易，所以最近也就没有那么热了。
“肥胖”网络的隐藏层数较少，如上左图。虽然有研究表明，浅而肥的网络也可以拟合任何的函数，但它需要非常的“肥胖”，可能一层就要成千上万个神经元。而这直接导致的后果是参数的数量增加到很多很多。

也有实验表明，也就是上图的实验，我们可以清楚的看出，**当准确率差不多的时候，参数的数量却相差数倍**。这也说明我们一般用深层的神经网络而不是浅层“肥胖”的网络。

> 注意：说神经网络多少层数的时候一般不包括输入层。 在神经网络中的激活主要讲的是梯度的更新的激活
#### 为什么要有激活函数
就是使得神经网络**具有的拟合非线性函数**的能力，使得其具有强大的表达能力！

简单扩展，神经网络的万能近似定理:一个前馈神经网络如果具有线性层和至少一层具有"挤压"性质的激活函数（如signmoid等），给定网络足够数量的隐藏单元，它可以以任意精度来近似任何从一个有限维空间到另一个有限维空间的borel可测函数。

要相符上面的定理，也就是想拟合任意函数，一个必须点是“要有带有“挤压”性质的激活函数”。这里的“挤压”性质是因为早期对神经网络的研究用的是sigmoid类函数，所以对其数学性质的研究也主要基于这一类性质：将输入数值范围挤压到一定的输出数值范围。（后来发现，其他性质的激活函数也可以使得网络具有普适近似器的性质，如ReLU 。
#### 选择激活函数
## sigmoid

优点：有较好的解释性

缺点：1.Sigmoid函数饱和使梯度消失。sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。2.输出不是零中心的，这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数，那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这样梯度下降权重更新时出现z字型的下降。这样收敛会变得异常的慢。（这也是为什么要一直保持为数据的0中心化）—–但这个问题比较小3.exp（）在深度神经网络时候相比其他运算就比较慢

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/lFAia5dnhjrWwb7msltgdgwAbPmz8o8hRf5QnWibiaUG1poPdQKN2BxlbjzU6TwmfQ90rRbibAf69Llc42eF9wVuQw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)Tanh非线性函数

优点：1.它的输出是零中心的。因此，在实际操作中，tanh非线性函数比sigmoid非线性函数更受欢迎。

缺点：1.和Sigmoid函数一样，饱和使梯度消失。计算慢

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/lFAia5dnhjrWwb7msltgdgwAbPmz8o8hRyuQBueVadx504d7p93tBuP8stuutmgad4wCaw7YPfucD2tmyZh19NA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)ReLU

优点：1.ReLU对于随机梯度下降的收敛有巨大的加速作用（ Krizhevsky 等的论文alexnet指出有6倍之多）。据称这是由它的线性，非饱和的公式导致的；2.注意：现在大部分的DNN用的激活函数就是ReLu

缺点：1.当x是小于0的时候，那么从此所以流过这个神经元的梯度将都变成0；这个时候这个ReLU单元在训练中将死亡（也就是参数无法更新），这也导致了数据多样化的丢失（因为数据一旦使得梯度为0，也就说明这些数据已不起作用）。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/lFAia5dnhjrWwb7msltgdgwAbPmz8o8hRsZgXWF89HYD1IqkiagYaZmpgVYNN1ew695CXrznVECVdEKOdSuo3wtw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)Leaky ReLU

优点：1.非饱和的公式;2.Leaky ReLU是为解决“ReLU死亡”问题的尝试

缺点：1.有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定

> Kaiming He等人在2015年发布的论文Delving Deep into Rectifiers中介绍了一种新方法PReLU，把负区间上的斜率当做每个神经元中的一个参数。然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/lFAia5dnhjrWwb7msltgdgwAbPmz8o8hRUOibicns4uabyG3F9hrdPqaib4gLtFnYnwP1bxMP082zE80j30NLd5n1w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)ELU

指数线性单元（Exponential Linear Units, ELU） ELU的公式为：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/lFAia5dnhjrWwb7msltgdgwAbPmz8o8hRPIx1hneoUaHVf5sDI7FA61MSnI2JicjbPibVajwTrKuDbI2jVHYcW8EA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

ELU.png 函数曲线如下：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/lFAia5dnhjrWwb7msltgdgwAbPmz8o8hRbkVGmEB8pttgBHGROy50yf8dpRHcqCiaAllcaHpTeYkwccficZycQvBA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)Maxout

-   Maxout是对ReLU和leaky ReLU的一般化归纳
    

优点：1.拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）

缺点 ：1.每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。难训练,容易过拟合

#### 怎么用激活函数:
“那么该用那种呢？”用ReLU非线性函数。注意设置好学习率，(如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。),_解决方案：_或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。

## CNN卷积神经网络
主要用来处理图像，分为三层，卷积层提取图像特征，池化层降维防止过拟合，全连接层是神经网络层。
卷积层使用过滤器（卷积核，可能一个或者多个）遍历整张图，提取局部特征
因为卷积核本身比较小，所以卷积层之后还要继续使用池化层降维。
## RNN循环神经网络
主要用来处理序列数据（相互直觉有关联的数据）包括NLP，视频音频。理论基础：将上一层的输出结果带入下一层的隐藏层计算，缺点是短期记忆影响较大（尤其是在长时期的数据上），训练时间较长。为了解决短期记忆的影响产生了LSTM长短期记忆网络，通过划重点的方法标注出那些数据比较重要

## 深度强化学习 – RL

强化学习算法的思路非常简单，以游戏为例，如果在游戏中采取某种策略可以取得较高的得分，那么就进一步「强化」这种策略，以期继续取得较好的结果。这种策略与日常生活中的各种「绩效奖励」非常类似。我们平时也常常用这样的策略来提高自己的游戏水平。

在 Flappy bird 这个游戏中，我们需要简单的点击操作来控制小鸟，躲过各种水管，飞的越远越好，因为飞的越远就能获得更高的积分奖励。

这就是一个典型的强化学习场景：

-   机器有一个明确的小鸟角色——代理
-   需要控制小鸟飞的更远——目标
-   整个游戏过程中需要躲避各种水管——环境
-   躲避水管的方法是让小鸟用力飞一下——行动
-   飞的越远，就会获得越多的积分——奖励
### LSTM
参考链接
https://zhuanlan.zhihu.com/p/32085405 
https://easyai.tech/ai-definition/lstm/
https://zhuanlan.zhihu.com/p/42717426

## GAN 对抗神经网络
https://easyai.tech/ai-definition/gan/
设计初衷是自动化提取特征。形象化的描述就是警察和罪犯对抗，魔高一尺道高一丈，互相对抗，各自变强。生成器（罪犯）生成虚拟数据试图骗过判别器，判别器（警察）判断数据究竟是真实数据还是生成器生成的数据。具体就是循环：1 固定生成器，训练判别器，2 固定判别器训练生成器
缺点是：
1.  难训练，不稳定。生成器和判别器之间需要很好的同步，但是在实际训练中很容易D收敛，G发散。D/G 的训练需要精心的设计。
2.  模式缺失（Mode Collapse）问题。GANs的学习过程可能出现模式缺失，生成器开始退化，总是生成同样的样本点，无法继续学习。
## GNN 图神经网络
## GCN 图卷积网络
## LM
https://blog.csdn.net/boksic/article/details/79177055

### 预训练模型

通常而言，预训练好的网络参数，尤其是底层的网络参数，若抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。
目前主要有两种预训练语言模型用于下游任务的方法：feature-based（以ELMo为例）和fine-tuning（以BERT为例）

Feature-based
指使用预训练语言模型训练出的词向量作为特征，输入到下游目标任务中。

Fine-tuning
指在已经训练好的语言模型的基础上，加入少量的 task-specific parameters【只改变较少的任务相关的层与参数】，然后在新的语料上重新训练来进行微调【直接对下游任务训练整个原来的语言模型？？能说清楚具体是什么样的操作吗？】。
https://zhuanlan.zhihu.com/p/112929902
https://blog.csdn.net/liu16659/article/details/108059710

## 数据预处理
### 数据清洗
### 归一化与空值处理
### 数据增强
### Mask
### 多目标预测
做数据增强，做mask，做多目标预测
